{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEHC0zbKbUkV0L8Bqoi/+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjosh003-CS/Data-Mining-coursework/blob/main/DM_Coursework_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Data Mining Assignment</b><br>\n",
        "\n",
        "<b>Opened:</b> Tuesday, 20 February 2024, 12:00 AM<br>\n",
        "<b>Due:</b> Friday, 22 March 2024, 12:00 PM\n",
        "</b>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "<b>Data Mining Assignment</b> <br>\n",
        "\n",
        "This assignment represents 100% of the Data Mining module’s mark. It is composed of Part 1 which is worth 40 marks, and Part 2 which is worth 60 marks. You can work in a team of 2 students for this assignment. One student per team will be chosen by the team as being the team leader – who will be in charge of coordinating the team’s work, and of submitting the assignment in their account on VLE on behalf of all the team.\n",
        "\n",
        "<b>PART 1:</b>\n",
        "\n",
        "This task is based on the Sonar real data seen previously in class. Several objects which can be rock or metal cylinders are scanned on different angles and under different conditions, with sonar signals. 60 measurements are recorded per columns for each object (one record per object) and these are the predictors called A1, A2, …, A60. The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is metal cylinder, and this is the outcome variable called Class.\n",
        "\n",
        "Two datasets are provided to you: a training dataset in the sonar_train.csv file, and a test dataset in the sonar_test.csv file.\n",
        "\n",
        "a) You are required to write a Python code implementing the simple Nearest Neighbour algorithm, with the Minkowski distance, both discussed in lecture of week 1. You should not implement k-Nearest Neighbour, for an arbitrary number of neighbours k. Your code will read the power q appearing in the Mionkowski distance, and will classify each record from the test dataset based on the training dataset. Remember, to classify a record from the test set you need to find its nearest neighbour in the training set (this is the one which minimizes the distance to the test set record); take the class of the nearest neighbour as the predicted class for the test set record. After classifying all the records in the test set, your code needs to calculate and display the accuracy, recall, precision, and F1 measure with respect to the class \"M\" (which is assumed to be the positive class), of the predictions on the test dataset. Run your code to produce results for Manhattan and for Euclidian distances, which are particular cases of Minkowski's distance.\n",
        "\n",
        "b) Run your code for the power q as a positive integer number from 1 to 20 and display the accuracy, recall, precision, and F1 measure on the test set in a chart. Which value of q leads to the best accuracy on the test set?\n",
        "\n",
        "The code, comments, explanations and results will be provided in a Jupyter notebook called Part1.\n",
        "\n",
        "<u>Note that in this task you are not to apply a library for the nearest neighbour algorithm, but you are required to compute the distances, find the nearest neighbour, and so code yourself this simple algorithm.</u>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Note: </b> regarding working in a team or individually, and what you need to submit:\n",
        "\n",
        "  * <b>You can work and submit in a team of 2 students </b>- in which case you should choose a team leader.  As a team you should work on all the tasks. Include the names and student numbers of both of the team members on top of each notebooks Part 1 and Part 2, and indicate who is the team leader. The team leader must perform the submission from their account (hence only once) for both students.\n",
        "\n",
        "  * <b>Or you can work also work and submit alone </b>for this coursework. In this case you must tackle only point (a) in Part 1, and only 3 out of the 6 algorithms mentioned in Part 2 (at your choice, but **choose 3 only**). Include your name and student number on top of the notebooks Part 1 and Part 2,  followed by the mention **\"I worked and submitted alone\"**"
      ],
      "metadata": {
        "id": "uoPtBXqk1fMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Coursework summary\n",
        "\n",
        "**Part 1:**\n",
        "- **Objective:** Implement the Nearest Neighbour algorithm with Minkowski distance on the Sonar dataset to classify objects as rock or metal cylinders.\n",
        "- **Tasks:**\n",
        "  - Implement Nearest Neighbour algorithm with Minkowski distance.\n",
        "  - Classify test dataset records based on the training dataset.\n",
        "  - Calculate accuracy, recall, precision, and F1 measure for different values of q (power in Minkowski distance).\n",
        "  - Run the code for Manhattan and Euclidean distances.\n",
        "- **Submission Requirements:** A Jupyter notebook named Part1 with code, comments, explanations, and results displayed.\n",
        "\n",
        "**Part 2:**\n",
        "- **Objective:** Build the best predictive model for credit card default prediction using various machine learning algorithms.\n",
        "- **Tasks:**\n",
        "  - Use k-Nearest Neighbours, Decision Trees, Random Forest, Bagging, AdaBoost (or XGBoost), and SVM algorithms.\n",
        "  - Tune models using cross-validation on the training dataset.\n",
        "  - Select the best model and justify the choice.\n",
        "  - Evaluate the best model's performance on the test set.\n",
        "  - Include charts illustrating how accuracy varies with one numeric hyperparameter for each algorithm.\n",
        "- **Submission Requirements:** A Jupyter notebook named Part2 with code, comments, explanations, and results displayed. Additionally, include charts for hyperparameter tuning.\n",
        "\n",
        "**Submission Details:**\n",
        "- **Teamwork:** Work in teams of two, with one team leader coordinating the submission.\n",
        "- **Individual Work:** If working alone, only tackle Part 1 and three algorithms from Part 2.\n",
        "- **Submission:** Team leader submits the assignment from their account, mentioning both team members' names and student numbers at the top of each notebook. If working alone, include a note indicating so.\n",
        "- **Deadline:** Ensure timely submission as per the course guidelines.\n",
        "\n",
        "Both parts emphasize coding, explanation, and analysis, with the second part requiring the exploration of multiple machine learning algorithms and model tuning for predictive performance."
      ],
      "metadata": {
        "id": "U_Pbd4-a1h1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<h1></h1>\n",
        "<h1 align = center><b>Coursework</b><h1>\n",
        "<h2 align = center><u>Introduction to Data Mining (2023-2024)</u></h2>\n",
        "\n",
        "<h3 align = center><i>Part 1: Sonar Data</i></h3>\n",
        "\n",
        "<br>\n",
        "<h4 align = center>Student:  <span style=\"color:blue\">Rohit Joshi</span></h4>\n",
        "<h4 align = center>Student ID:  <span style=\"color:blue\">33726546</span></h4>\n",
        "<h4 align = center>Submission Date:  <span style=\"color:blue\">22th March 2024</span></h4>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LnwVKHnhw6Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Introduction"
      ],
      "metadata": {
        "id": "JjO70jOFzAWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Definition of the problem\n",
        "\n",
        " Implement the Nearest Neighbour algorithm with Minkowski distance on the Sonar dataset to classify objects as rock or metal cylinders.\n",
        "\n",
        " * Implement Nearest Neighbour algorithm with Minkowski distance.\n",
        "\n",
        "* Classify test dataset records based on the training dataset.\n",
        "\n",
        "* Calculate accuracy, recall, precision, and F1 measure for different values of q (power in Minkowski distance).\n",
        "\n",
        "* Run the code for Manhattan and Euclidean distances.\n"
      ],
      "metadata": {
        "id": "K4RCs9THzFA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 About the Dataset\n",
        "\n",
        "The task involves analyzing the Sonar real data, which consists of scanning several objects, including rock and metal cylinders, at various angles and under different conditions using sonar signals.\n",
        "\n",
        "<br>\n",
        "\n",
        "Each object's sonar signals are recorded as 60 measurements, labeled as A1 through A60. The label associated with each record indicates whether the object is a rock (denoted by \"R\") or a metal cylinder (denoted by \"M\"), serving as the outcome variable named Class.\n",
        "\n",
        "<br>\n",
        "\n",
        "The dataset is divided into a training set and  test set;\n",
        "\n",
        "* **training set:**  containing 139 records with 60 features (different angles and different conditions)\n",
        "\n",
        "* **test set:** consisting of 69 records with 60 features (different angles and different conditions). The training dataset is stored in the sonar_train.csv file, while the test dataset is stored in the sonar_test.csv file.\n",
        "\n"
      ],
      "metadata": {
        "id": "gOUSsBAqzV_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Evaluation Metrics"
      ],
      "metadata": {
        "id": "aNWw9y72zcsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluating the performance of the nearest neighbor classifier implemented in Part 1, we can use several evaluation metrics commonly used in classification tasks. Here are some of the key evaluation metrics we can calculate:\n",
        "\n",
        "1. **Accuracy**: It measures the proportion of correctly classified instances out of the total instances. It's calculated as:\n",
        "\n",
        "   $$\n",
        "   \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
        "   $$\n",
        "\n",
        "2. **Precision**: It measures the proportion of true positive predictions out of all positive predictions. It's calculated as:\n",
        "   \n",
        "   $$\n",
        "   \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "   $$\n",
        "\n",
        "3. **Recall (Sensitivity)**: It measures the proportion of true positive predictions out of all actual positive instances. It's calculated as:\n",
        "   \n",
        "   $$\n",
        "   \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "   $$\n",
        "\n",
        "4. **F1 Score**: It is the harmonic mean of precision and recall, providing a single score that balances both metrics. It's calculated as:\n",
        "   \n",
        "   $$\n",
        "   \\text{F1 Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "   $$\n",
        "\n"
      ],
      "metadata": {
        "id": "Q93O0BYpsaQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Data Processing"
      ],
      "metadata": {
        "id": "d2PBXRl7zuSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the steps for Data Processing for part 1:\n",
        "\n",
        "1. **Loading the Data**\n",
        "\n",
        "    Load the training and test datasets from the provided CSV files (sonar_train.csv and sonar_test.csv).\n",
        "\n",
        "\n",
        "2. **Exploratory Data Analysis (EDA)**\n",
        "\n",
        "    Explore the datasets to understand the structure and characteristics of the data.\n",
        "    Check for missing values, data types, and summary statistics.\n",
        "    Visualize the data distribution and class balance.\n",
        "\n",
        "\n",
        "3. **Preprocessing**\n",
        "\n",
        "    Handle missing values (if any).\n",
        "    Encode categorical variables (if any).\n",
        "    Split features (predictors) and target variable.\n",
        "\n",
        "\n",
        "4. **Feature Scaling (if required)**\n",
        "\n",
        "    Standardize or normalize numerical features to ensure they have the same scale.\n",
        "\n",
        "\n",
        "5. **Model Training and Evaluation**\n",
        "\n",
        "    Implement the Nearest Neighbour algorithm with the Minkowski distance.\n",
        "    Evaluate the model using accuracy, recall, precision, and F1 score.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Xx7ecV_aufMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Data Pre-processing"
      ],
      "metadata": {
        "id": "QARYJ8gTzuIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required Packages"
      ],
      "metadata": {
        "id": "6FjECYzuUhRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# built-in package\n",
        "import random\n",
        "\n",
        "# Ipython package\n",
        "from IPython.display import display\n",
        "\n",
        "# sklearn package\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# sklearn - utils\n",
        "# from sklearn import multioutput\n",
        "# from sklearn import clone\n",
        "# from sklearn import preprocessing, pipeline\n",
        "\n",
        "# sklarn - metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
        "\n",
        "# sklearn - models\n",
        "from sklearn.ensemble import  RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "# from sklearn.ensemble import  RandomForestRegressor\n",
        "# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "# from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
        "# from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
        "\n",
        "# xgboost model\n",
        "# from xgboost import XGBRegressor, XGBClassifier\n",
        "\n",
        "\n",
        "# numpy packages\n",
        "import numpy as np\n",
        "\n",
        "# matplotlib packages\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# seaborn packages\n",
        "import seaborn as sns\n",
        "\n",
        "# pandas packages\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#  Setting seeds for data reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ],
      "metadata": {
        "id": "Myfdr_AqUm_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "v25eOvrUM8hM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to load the github repository into the colab drive for getting the samples."
      ],
      "metadata": {
        "id": "CXeKpnP9NMr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "class GIT():\n",
        "  \"\"\"\n",
        "    description: class to initializes a git object and load the git hub repos\n",
        "    for the training and test data.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, local_path = None, repo_url = None):\n",
        "    self.github_token = userdata.get('github_token')\n",
        "    self.username = userdata.get('username')\n",
        "    self.dir_name = userdata.get('dir_name')\n",
        "    self.repo_name= userdata.get('repo_name')\n",
        "    self.clone_done = False\n",
        "\n",
        "    print(f\"github_token: {self.github_token}\")\n",
        "    print(f\"username: {self.username}\")\n",
        "    print(f\"dir_name: {self.dir_name}\")\n",
        "    print(f\"repo_name: {self.repo_name}\")\n",
        "\n",
        "    # Construct the repository URL and local path\n",
        "    if repo_url == None:\n",
        "      self.repo_url = f\"https://github.com/{self.username}/{self.repo_name}.git\"\n",
        "    else:\n",
        "      self.repo_url = repo_url\n",
        "\n",
        "    if local_path == None:\n",
        "      self.local_path = f\"/content/{self.repo_name}\"\n",
        "    else:\n",
        "      self.local_path = local_path\n",
        "\n",
        "    print(f\"repo_url: {self.repo_url}\")\n",
        "    print(f\"local_path: {self.local_path}\")\n",
        "\n",
        "\n",
        "  def clone_git(self):\n",
        "      if not self.clone_done:\n",
        "        # Clone the repository using the token for authentication\n",
        "        !git clone {self.repo_url} {self.local_path}\n",
        "        !git config --global credential.helper store\n",
        "        !cd {local_path} && git checkout master\n",
        "        self.clone_done = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "git = GIT()\n",
        "\n",
        "# another method that clones the data from the github_repo\n",
        "def clone_data(clone_needed = True):\n",
        "    if clone_needed:\n",
        "      not_clonned = True\n",
        "      if not_clonned:\n",
        "          git.clone_git()\n",
        "          not_clonned = False\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FKd1Y79e1r2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions that are used to get the dir_path for the sample files"
      ],
      "metadata": {
        "id": "POtp8rQaM_tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from typing import Union\n",
        "from os import path\n",
        "\n",
        "class Data():\n",
        "\n",
        "# contructor\n",
        "    def __init__(self, is_github_repo = True, repo_name: str = None):\n",
        "      self.__is_github_repo = is_github_repo\n",
        "\n",
        "      repo_name = userdata.get('repo_name')\n",
        "\n",
        "      if is_github_repo:\n",
        "        self.__dir_path = f\"/content/{repo_name}/data\"\n",
        "      else:\n",
        "        self.__dir_path = \"./\"\n",
        "\n",
        "      self.__test_path = None\n",
        "      self.__train_path = None\n",
        "\n",
        "\n",
        "# method: returns a dir_path\n",
        "    def get_dir_path(self):\n",
        "      \"\"\"\n",
        "        method: get_dir_path\n",
        "        params: None\n",
        "        returns: self.dir_path\n",
        "        description: it returns the dir.path stored inside the class\n",
        "      \"\"\"\n",
        "      return self.__dir_path\n",
        "\n",
        "\n",
        "    def get_file_path(self, file_names: Union[str,list]= None):\n",
        "\n",
        "      \"\"\" method: get_file_path\n",
        "          params: file_name\n",
        "          returns: the completee filepath for the stated files\n",
        "      \"\"\"\n",
        "\n",
        "      if file_names is None:\n",
        "        raise ValueError(f\"Enter a file name: \\ndir: {self.__dir_path}\")\n",
        "\n",
        "      if isinstance(file_names, str):\n",
        "        return path.join(self.__dir_path, file_names)\n",
        "\n",
        "      if isinstance(file_names, list):\n",
        "        f_paths = []\n",
        "        for f_name in file_names:\n",
        "          f_paths.append(path.join(self.__dir_path, f_name))\n",
        "\n",
        "        return f_paths\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a2OhyleOLmCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load the data from the git hub repos\n",
        "def load_github_data():\n",
        "  is_github_repo = True\n",
        "\n",
        "  test_file_name =  \"sonar_test.csv\"\n",
        "  train_file_name = \"sonar_train.csv\"\n",
        "\n",
        "  data = Data(is_github_repo = is_github_repo)\n",
        "  train_file, test_file = data.get_file_path(file_names = [train_file_name ,test_file_name])\n",
        "\n",
        "  print(f\"train_file: {train_file}\")\n",
        "  print(f\"test_file: {test_file}\")\n",
        "\n",
        "  return pd.read_csv(train_file),pd.read_csv(test_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load training and test datasets\n",
        "def load_local_data():\n",
        "  train_data = pd.read_csv('sonar_train.csv')\n",
        "  test_data = pd.read_csv('sonar_test.csv')\n",
        "  return train_data, test_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  load data from github or the from working dir\n",
        "def load_data(load_from_github = True):\n",
        "  if(load_from_github):\n",
        "    # cloning the data from the github repo\n",
        "    clone_data()\n",
        "    # loading the data as pandas dataframes\n",
        "    return load_github_data()\n",
        "  else:\n",
        "    return load_local_data()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GPJ5U-_4LzpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pandas"
      ],
      "metadata": {
        "id": "xFvlM9QI45GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to check for the missing values in the data set if any\n",
        "def check_missing_values(df, msg = \"data set\"):\n",
        "  \"\"\"\n",
        "    method: check_missing_values\n",
        "    params:\n",
        "            df: dataframe\n",
        "            msg: name of the dataframe\n",
        "            description: method checks of any missing values for a 2D dataframe\n",
        "  \"\"\"\n",
        "  print( \"any missing values is \" + msg + \": \",((train_data.isna()).all()).all())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function to return the shape of the dataframe\n",
        "def shape(**kwargs):\n",
        "  \"\"\"\n",
        "    method: shape(**kwargs)\n",
        "    params: **kwargs takes in a dictionary (key: value pairs)\n",
        "    description:\n",
        "\n",
        "            \"takes in a key_value pair which is a pandas Dataframe\"\n",
        "\n",
        "            key: is the name of the pandas.Dataframe\n",
        "            values: is the shape pandas.Dataframe\n",
        "  \"\"\"\n",
        "  for name, value in kwargs.items():\n",
        "    print(f\"shape of {name}: {value.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function to concatenate the desired feature columns and targets\n",
        "def concat_columns(df: pd.DataFrame, features: list, targets: list):\n",
        "  \"\"\"\n",
        "    method: concat_columns\n",
        "    params:\n",
        "      df: pd.DataFrame\n",
        "      features: list of features names\n",
        "      targets: list of targets names\n",
        "\n",
        "    description: It takes a pandas dataframe and returns the columns and targets\n",
        "      as a new dataframe.\n",
        "  \"\"\"\n",
        "  return pd.concat([df[features], df[targets]], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enRZBSul44j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minkowski distance formulae"
      ],
      "metadata": {
        "id": "m9fYo__7dEzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate Minkowski distance [1]\n",
        "def minkowski_distance(x1, x2, q):\n",
        "    return np.power(np.sum(np.abs(x1 - x2) ** q), 1/q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to perform simple Nearest Neighbour classification [1]\n",
        "def nearest_neighbour(train_data, test_data, q):\n",
        "    predictions = []\n",
        "\n",
        "    for test_index, test_row in test_data.iterrows():\n",
        "        min_dist = float('inf')\n",
        "        nearest_class = None\n",
        "\n",
        "        for train_index, train_row in train_data.iterrows():\n",
        "            dist = minkowski_distance(train_row[:-1], test_row[:-1], q)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                nearest_class = train_row['Class']\n",
        "\n",
        "        predictions.append(nearest_class)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to calculate evaluation metrics [1]\n",
        "def calculate_metrics(true_labels, predicted_labels):\n",
        "    tp = sum((true == 'M') and (pred == 'M') for true, pred in zip(true_labels, predicted_labels))\n",
        "    fp = sum((true == 'R') and (pred == 'M') for true, pred in zip(true_labels, predicted_labels))\n",
        "    tn = sum((true == 'R') and (pred == 'R') for true, pred in zip(true_labels, predicted_labels))\n",
        "    fn = sum((true == 'M') and (pred == 'R') for true, pred in zip(true_labels, predicted_labels))\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    recall = tp / (tp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return accuracy, recall, precision, f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate metrics for different values of q [1]\n",
        "def cal_scores_for_qs(qs, train_data = None, test_data = None):\n",
        "  results = {'q': [], 'accuracy': [], 'recall': [], 'precision': [], 'f1_score': []}\n",
        "  for q in qs:\n",
        "      predictions = nearest_neighbour(train_data, test_data, q)\n",
        "      accuracy, recall, precision, f1_score = calculate_metrics(test_data['Class'], predictions)\n",
        "      results['q'].append(q)\n",
        "      results['accuracy'].append(accuracy)\n",
        "      results['recall'].append(recall)\n",
        "      results['precision'].append(precision)\n",
        "      results['f1_score'].append(f1_score)\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function to plot the evaluation metrics for the results of the nearest_neighbours on q values [1]\n",
        "def plot_evaluation_metric(results_df = None):\n",
        "  if results_df is not None:\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['q'], results_df['accuracy'], label='Accuracy')\n",
        "    plt.plot(results_df['q'], results_df['recall'], label='Recall')\n",
        "    plt.plot(results_df['q'], results_df['precision'], label='Precision')\n",
        "    plt.plot(results_df['q'], results_df['f1_score'], label='F1 Score')\n",
        "    plt.title('Evaluation Metrics vs. q')\n",
        "    plt.xlabel('Value of q')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(range(1, 21))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7HPWJyE_AamH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function get the manhattan distance from the Minkowski's distance\n",
        "def manhattan(train_data, test_data):\n",
        "  qs = [1]\n",
        "  results = cal_scores_for_qs(qs, train_data, test_data)\n",
        "  # Display results\n",
        "  results_df = pd.DataFrame(results)\n",
        "  results_df.set_index('q', inplace = True)\n",
        "  return results_df\n",
        "\n",
        "\n",
        "\n",
        "# function get the manhattan distance from the Euclidian distance\n",
        "def euclidian(train_data, test_data):\n",
        "  qs = [2]\n",
        "  results = cal_scores_for_qs(qs, train_data, test_data)\n",
        "  # Display results\n",
        "  results_df = pd.DataFrame(results)\n",
        "  results_df.set_index('q', inplace = True)\n",
        "  return results_df\n",
        "\n"
      ],
      "metadata": {
        "id": "UQ1_x22wDmtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Data Loading"
      ],
      "metadata": {
        "id": "sJ0Qx90vzsEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = None, None\n",
        "\n",
        "# set it to false for local dir: ./sonar_data\n",
        "train_data, test_data = load_data()\n",
        "\n",
        "# displaying the data\n",
        "display(train_data.head())\n",
        "display(test_data.head())"
      ],
      "metadata": {
        "id": "2pPKMduBMp2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"shape of train_data: {train_data.shape}\")\n",
        "print(f\"shape of test_data: {test_data.shape}\")"
      ],
      "metadata": {
        "id": "oCpn5T9L78re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape(train_data=train_data, test_data=test_data)"
      ],
      "metadata": {
        "id": "aasdfK9e9RRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "XSxZPjgCRI2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.info()"
      ],
      "metadata": {
        "id": "nLBAZJBcRYMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "qRk7cvlORiRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.describe()"
      ],
      "metadata": {
        "id": "YV5SD_szRnlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_series = train_data.drop(\"Class\", axis = 1).stack()\n",
        "print(flat_series.min())\n",
        "print(flat_series.max())"
      ],
      "metadata": {
        "id": "NsWhsCwCN26y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_series = test_data.drop(\"Class\", axis = 1).stack()\n",
        "print(flat_series.min())\n",
        "print(flat_series.max())"
      ],
      "metadata": {
        "id": "5AI577s5R31n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Cleaning"
      ],
      "metadata": {
        "id": "U8brMRob0nGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking whether the data contain any missing values."
      ],
      "metadata": {
        "id": "2RU-gjp4xwGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_missing_values(train_data, \"train_data\")\n",
        "check_missing_values(test_data, \"test_data\")"
      ],
      "metadata": {
        "id": "PoIyzVub4_Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the train_data does not contain any missing values."
      ],
      "metadata": {
        "id": "7oey7kOZx_Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the Training Data for Simple Models"
      ],
      "metadata": {
        "id": "YM7k4teiEmm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "train_X = train_data.drop(columns=['Class'])  # Features\n",
        "train_target = train_data['Class']  # Target variable\n",
        "train_y = (train_target == \"M\").astype(\"uint8\")\n",
        "\n",
        "test_X = test_data.drop(columns=[\"Class\"]) # Features\n",
        "test_target = test_data[\"Class\"] # Target variable\n",
        "test_y = (test_target == \"M\").astype(\"uint8\")"
      ],
      "metadata": {
        "id": "MLGWj8upEQdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_X.head())\n",
        "shape(train_X=train_X)\n",
        "\n",
        "print()\n",
        "display(train_target.head())\n",
        "shape(train_target=train_target)\n",
        "\n",
        "print()\n",
        "display(train_y.head())\n",
        "shape(train_y=train_y)"
      ],
      "metadata": {
        "id": "vBbRQdqyEX5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(test_X.head())\n",
        "shape(test_X=test_X)\n",
        "\n",
        "print()\n",
        "display(test_target.head())\n",
        "shape(test_target=test_target)\n",
        "\n",
        "print()\n",
        "display(test_y.head())\n",
        "shape(test_y=test_y)"
      ],
      "metadata": {
        "id": "f8eEv1ByJNm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_indices = nearest_neighbors(train_X.values, train_y.values,test_X.values)\n",
        "np.array(nearest_indices)"
      ],
      "metadata": {
        "id": "F9f0O77rHnY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Features Engineering"
      ],
      "metadata": {
        "id": "byICrybP0slg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Using Heat Maps and Correlation Matrix"
      ],
      "metadata": {
        "id": "SwPQsx2DHgzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_1 = train_data.copy()\n",
        "metalic = (train_data_1[\"Class\"] == \"M\").astype(np.uint8)\n",
        "train_data_1[\"Class\"] = (np.zeros(len(train_data_1)) + metalic).astype(np.uint8)\n",
        "train_data_1"
      ],
      "metadata": {
        "id": "VhMBA9c7KGIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix\n",
        "correlation_matrix = train_data_1.iloc[:,:62].corr()"
      ],
      "metadata": {
        "id": "qUWBxIh5HgLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix"
      ],
      "metadata": {
        "id": "VwYxDcFPJo2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x86uBC7VIR1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a correlation threshold\n",
        "correlation_threshold = 0.36\n",
        "\n",
        "# Find features highly correlated with the target variable 'Class'\n",
        "highly_correlated_features = correlation_matrix['Class'][abs(correlation_matrix['Class']) > correlation_threshold].index.tolist()\n",
        "\n",
        "# Remove the target variable from the list\n",
        "highly_correlated_features.remove('Class')\n",
        "\n",
        "# Print the highly correlated features\n",
        "print(\"Highly Correlated Features:\", highly_correlated_features)\n",
        "\n",
        "# Drop the highly correlated features from the dataset\n",
        "train_data_filtered = train_data.drop(columns=highly_correlated_features)\n",
        "\n",
        "# Print the shape of the filtered dataset\n",
        "print(\"Shape of Filtered Dataset:\", train_data_filtered.shape)"
      ],
      "metadata": {
        "id": "u4NdbMxqIWui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see we can have approximately `5 features` that are `highly correlated with the target` at a `threshold` around `0.36`.\n",
        "\n",
        "Thus, by adjusting the threshold we could reduce the features dimesion from 60 to 5 based on how much correlation we want for the data to have."
      ],
      "metadata": {
        "id": "z6YVJCUs_KiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2 Using PCA (Princpal Component Analysis)"
      ],
      "metadata": {
        "id": "aELMxp3v9ENd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets find the cumulative variance of features that is greater than 90% of the total variance"
      ],
      "metadata": {
        "id": "Kt82nG6aNAip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(pca.explained_variance_ratio_, bins = 20)\n",
        "plt.title(\"explained variance ratio\", fontsize = 14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1u30UG3WNSx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Standardize the data (assuming X is your feature matrix)\n",
        "# You can use StandardScaler or MinMaxScaler for this purpose\n",
        "# X_std = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(train_X)\n",
        "\n",
        "# Calculate cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance ratio\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "percentile_ratio = 0.90\n",
        "# Find the number of components that explain a desired amount of variance (e.g., 90%)\n",
        "n_components = np.argmax(cumulative_variance_ratio >= percentile_ratio) + 1\n",
        "print(f\"Number of components explaining {percentile_ratio}% variance:\", n_components)\n"
      ],
      "metadata": {
        "id": "eYK_eZGNEkd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above figure, shows that about 12 features cummulative variance actually makes the 90 % of the data set variance in the training data."
      ],
      "metadata": {
        "id": "rFH9zrvWd0mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a) Scatter plot of the Nearest Neighbours using 2D PCA plots using matplotlib.pyplot"
      ],
      "metadata": {
        "id": "OaDbnRB2CAea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming nearest_indices contains the indices of the nearest neighbors\n",
        "# and train_X contains the corresponding data points with n features\n",
        "\n",
        "# Extract the nearest neighbor data points\n",
        "nearest_points = train_X.iloc[nearest_indices]\n",
        "\n",
        "# Apply PCA to reduce the dimensionality to 2\n",
        "pca = PCA(n_components=2)\n",
        "train_X_2d = pca.fit_transform(train_X)\n",
        "test_X_2d = pca.transform(test_X)\n",
        "\n",
        "nearest_points_2d = pca.transform(nearest_points)\n",
        "\n",
        "# Scatter plot the nearest neighbor data points in 2D\n",
        "plt.scatter(train_X_2d[:, 0], train_X_2d[:, 1], label='All Data')\n",
        "plt.scatter(nearest_points_2d[:, 0], nearest_points_2d[:, 1], label='Nearest Neighbors', color='red')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Scatter Plot of Nearest Neighbors (2D PCA)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LDGE8yYQ4-zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above figure we cannot observe the depth between the two points."
      ],
      "metadata": {
        "id": "dzVG--dz1yyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b) Scatter plot of the Nearest Neighbours using 3D PCA plots using matplotlib.pyplot"
      ],
      "metadata": {
        "id": "DISt3M5F8-CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming nearest_indices contains the indices of the nearest neighbors\n",
        "# and train_X contains the corresponding data points with n features\n",
        "\n",
        "# Extract the nearest neighbor data points\n",
        "nearest_points = train_X.iloc[nearest_indices]\n",
        "\n",
        "# Apply PCA to reduce the dimensionality to 3\n",
        "pca = PCA(n_components=3)\n",
        "train_X_3d = pca.fit_transform(train_X)\n",
        "nearest_points_3d = pca.transform(nearest_points)\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot the original data\n",
        "ax.scatter(train_X_3d[:, 0], train_X_3d[:, 1], train_X_3d[:, 2], label='All Data')\n",
        "\n",
        "# Scatter plot the nearest neighbor data points\n",
        "ax.scatter(nearest_points_3d[:, 0], nearest_points_3d[:, 1], nearest_points_3d[:, 2], label='Nearest Neighbors', color='red')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "ax.set_title('Scatter Plot of Nearest Neighbors (3D PCA)')\n",
        "\n",
        "# Add legend\n",
        "ax.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wERQuadHgHgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above, figure we could se the plot of the nearest neighbours in depth.\n",
        "As we can't see the ploting by adjusting the orientation. Lets use plotly to plot it to look it in dynamic orientation."
      ],
      "metadata": {
        "id": "UBZkeEbU16wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c) Scatter plot of the Nearest Neighbours using 3D PCA plots using plotly"
      ],
      "metadata": {
        "id": "D1_hQhMA8Ukb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming nearest_indices contains the indices of the nearest neighbors\n",
        "# and train_X contains the corresponding data points with n features\n",
        "\n",
        "# Extract the nearest neighbor data points\n",
        "nearest_points = train_X.iloc[nearest_indices]\n",
        "\n",
        "n_components = 3\n",
        "\n",
        "# Apply PCA to reduce the dimensionality to 3\n",
        "pca = PCA(n_components=n_components)\n",
        "train_X_3d = pca.fit_transform(train_X)\n",
        "nearest_points_3d = pca.transform(nearest_points)\n",
        "\n",
        "# columns=['PC1', 'PC2', 'PC3', 'PC4']\n",
        "columns=[\"PC\" + str(i) for i in range(1, n_components + 1)]\n",
        "\n",
        "# Create a DataFrame for the original data\n",
        "df_train_X_3d = pd.DataFrame(train_X_3d, columns=columns)\n",
        "\n",
        "# Create a DataFrame for the nearest neighbor data points\n",
        "df_nearest_points_3d = pd.DataFrame(nearest_points_3d, columns=columns)\n",
        "\n",
        "# Create a 3D scatter plot using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Scatter plot the original data\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=df_train_X_3d['PC1'],\n",
        "    y=df_train_X_3d['PC2'],\n",
        "    z=df_train_X_3d['PC3'],\n",
        "    mode='markers',\n",
        "    marker=dict(color='blue'),\n",
        "    name='All Data'\n",
        "))\n",
        "\n",
        "# Scatter plot the nearest neighbor data points\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=df_nearest_points_3d['PC1'],\n",
        "    y=df_nearest_points_3d['PC2'],\n",
        "    z=df_nearest_points_3d['PC3'],\n",
        "    mode='markers',\n",
        "    marker=dict(color='red', size=8),\n",
        "    name='Nearest Neighbors'\n",
        "))\n",
        "\n",
        "# Set labels and title\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis_title='Principal Component 1',\n",
        "        yaxis_title='Principal Component 2',\n",
        "        zaxis_title='Principal Component 3',\n",
        "        aspectmode='cube'\n",
        "    ),\n",
        "    title='Scatter Plot of Nearest Neighbors (3D PCA)'\n",
        ")\n",
        "\n",
        "# Show plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "MYULioKFhX4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Using the Random Forest"
      ],
      "metadata": {
        "id": "bGCdrHgbCXd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_ranking(rf_model = None, n_features = 1):\n",
        "  if rf_model is not None:\n",
        "    # Get feature importances\n",
        "    importances = rf_model.feature_importances_\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    return indices[:n_features]\n"
      ],
      "metadata": {
        "id": "iSaMSEO4SWTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a) lets find the 5 best features from the data set using the random forest."
      ],
      "metadata": {
        "id": "zH8hVdA-SUYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=500, max_features=5, random_state=42)\n",
        "rfc.fit(train_X, train_y)"
      ],
      "metadata": {
        "id": "k4NjexHsOvU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_pred = rfc.predict(test_X)\n",
        "\n",
        "\n",
        "print(f\"accuracy :=   {accuracy_score(test_y, rfc_pred)}\")\n",
        "print(f\"precision :=  {precision_score(test_y, rfc_pred)}\")\n",
        "print(f\"recall :=     {recall_score(test_y, rfc_pred)}\")\n",
        "print(f\"f1 :=         {f1_score(test_y, rfc_pred)}\")"
      ],
      "metadata": {
        "id": "qKPybgkmPyXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_ranking(rfc, 5)"
      ],
      "metadata": {
        "id": "DosASssRSAhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best 5 features based on the correlation matrix:\\\n",
        "['A9', 'A10', 'A11', 'A48', 'A49']\n",
        "\n",
        "random forest best features indices:\\\n",
        "[10, 47,  9,  8, 48]\n",
        "\n",
        "radom forest best features columns:\\\n",
        "[A11, A48, A10, A9, A49]\n",
        "\n",
        "As we can see that the 5 best features are same from the correlation_matrix and the random forest model"
      ],
      "metadata": {
        "id": "smnzcsNuTpD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b) random forest with best 12 features."
      ],
      "metadata": {
        "id": "OzEPPfHyUvWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc1 = RandomForestClassifier(n_estimators=500, max_features=12, random_state=42)\n",
        "rfc1.fit(train_X, train_y)\n",
        "\n",
        "rfc_pred1 = rfc1.predict(test_X)\n",
        "\n",
        "print(f\"accuracy :=   {accuracy_score(test_y, rfc_pred1)}\")\n",
        "print(f\"precision :=  {precision_score(test_y, rfc_pred1)}\")\n",
        "print(f\"recall :=     {recall_score(test_y, rfc_pred1)}\")\n",
        "print(f\"f1 :=         {f1_score(test_y, rfc_pred1)}\")"
      ],
      "metadata": {
        "id": "Vt75fvEHVHBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks = feature_ranking(rfc, 12)\n",
        "ranks"
      ],
      "metadata": {
        "id": "u7tkp1YmVLit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_columns_12 = np.array(['A' + str(rank + 1) for rank in ranks])\n",
        "best_columns_12"
      ],
      "metadata": {
        "id": "HA_wbHVgWSSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c) Finding the best features with Random Forest using grid search"
      ],
      "metadata": {
        "id": "E_KL6XoKaKTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_features': np.arange(2,61)  # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=500,\n",
        "                            random_state=42)\n",
        "\n",
        "def custom_score(y_true, y_pred):\n",
        "  w= np.arange(4,0,-1)\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  return w[0] * f1 + w[1] * recall + w[2] * precision + w[3] * accuracy\n",
        "\n",
        "# Define F1 score as the metric for optimization\n",
        "# scorer = make_scorer(custom_score)\n",
        "scorer = make_scorer(f1_score)\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(train_X, train_y)\n",
        "\n",
        "# Get the best parameters and the best F1 score\n",
        "best_params = grid_search.best_params_\n",
        "best_f1_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best F1 score:\", best_f1_score)\n"
      ],
      "metadata": {
        "id": "8RYl_CFPacR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With custom_scores:**\n",
        "\n",
        "Best parameters: {'max_features': 3}\\\n",
        "Best F1 score: 8.319119686188653\n",
        "\n",
        "<br>\n",
        "\n",
        "**With f1_scores:**\n",
        "\n",
        "Best parameters: {'max_features': 3}\\\n",
        "Best F1 score: 0.8281992337164752"
      ],
      "metadata": {
        "id": "hehICtmBfYMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets find the best 3 params from random forest"
      ],
      "metadata": {
        "id": "6tVWu6w2ikmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc2 = RandomForestClassifier(n_estimators=500, max_features=3, random_state=42)\n",
        "rfc2.fit(train_X, train_y)\n",
        "\n",
        "rfc_pred2 = rfc1.predict(test_X)\n",
        "\n",
        "print(f\"accuracy :=   {accuracy_score(test_y, rfc_pred2)}\")\n",
        "print(f\"precision :=  {precision_score(test_y, rfc_pred2)}\")\n",
        "print(f\"recall :=     {recall_score(test_y, rfc_pred2)}\")\n",
        "print(f\"f1 :=         {f1_score(test_y, rfc_pred2)}\")\n",
        "\n",
        "ranks = feature_ranking(rfc, 3)\n",
        "print(f\"{ranks = }\")\n",
        "\n",
        "\n",
        "best_columns_3 = np.array(['A' + str(rank + 1) for rank in ranks])\n",
        "(f\"best columns: {best_columns_3}\")"
      ],
      "metadata": {
        "id": "jTdhuahAikFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Train-Validation-Test Split\n",
        "\n",
        "\n",
        "based on from the above:\n",
        "\n",
        "1. The data is already divided into train data and test data.\n",
        "\n",
        "2. The problem is classification of data using simple nearest neighbours.\n",
        "\n",
        "Thus, we don't need to train-test and train-validation split of the data."
      ],
      "metadata": {
        "id": "C3lxvvQ40sEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we can have a new train_X and test_X data with the best 3 columns as per the random forest. To reduces feature dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGuBiirqtRjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_12 = concat_columns(train_data, best_columns_12, [\"Class\"])\n",
        "test_data_12 = concat_columns(train_data, best_columns_12, [\"Class\"])"
      ],
      "metadata": {
        "id": "--5jj8w359GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_3 = concat_columns(train_data, best_columns_3, [\"Class\"])\n",
        "test_data_3 = concat_columns(test_data, best_columns_3, [\"Class\"])"
      ],
      "metadata": {
        "id": "W5O0P_TDtpwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_data_12)\n",
        "display(test_data_12)"
      ],
      "metadata": {
        "id": "sGcIEtVq6bPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_data_3)\n",
        "display(test_data_3)"
      ],
      "metadata": {
        "id": "k7DOqG9ut998"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Normalization/Scaling\n",
        "- **Feature Scaling:** Normalize/standardize input features to a similar scale to prevent dominance by certain features during model training.\n",
        "  - Techniques StandardScaler is applied to the features.\n",
        "\n",
        "As we can see from the above;\n",
        "\n",
        "1. The features data is already scaling between 0 and 1.\n",
        "\n",
        "Thus, we don't need to use Normalization on the data."
      ],
      "metadata": {
        "id": "NJ6FAhKh1lCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.6 Data Pipeline Creation (Optional)\n",
        "- **Pipeline Construction:** Construct a data processing pipeline to automate and streamline the preprocessing steps, ensuring consistency across different datasets and reducing code redundancy.\n",
        "\n",
        "As from the above:\n",
        "\n",
        "1. the data is already separated into train and test data.\n",
        "\n",
        "2. the data does not contain any missing values.\n",
        "\n",
        "3. the features scaling is also between 0 and 1.\n",
        "\n",
        "There is no need of train-test split or data Normalization. Thus, There is no need of Data pipeline."
      ],
      "metadata": {
        "id": "WQHxoFhE1jJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Save Processed Data (optional)\n",
        "- **Save Processed Data:** Save the processed datasets (training, validation, test) in a suitable format (CSV, HDF5, etc.) for easy access during model training and evaluation.\n",
        "\n",
        "As we can observe from the above:\n",
        "\n",
        "1. the data is just separated into features and targets for the train and test data.\n",
        "\n",
        "2. the given data does not have any missing values and does not need a scaling.\n",
        "\n",
        "3. the separation of the features and target does not require a huge computational time.\n",
        "\n",
        "As there is no data modification and no significant time difference to separate the features and the target data. We don't have any processed data to be save on.\n"
      ],
      "metadata": {
        "id": "4FECqslj0rvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Methodology"
      ],
      "metadata": {
        "id": "_cejLsIh21J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Manhattan Distance\n",
        "\n",
        "\n",
        "We can simply find the manhattan distance from the minskowski distance by just setting the value of q = 1."
      ],
      "metadata": {
        "id": "dCTeex8V31yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [1]\n",
        "results = cal_scores_for_qs(qs, train_data, test_data)"
      ],
      "metadata": {
        "id": "g7KkadRk3o3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.set_index('q', inplace = True)\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "BkBq5BG53zrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With features selection data\n",
        "\n",
        "Features = 3"
      ],
      "metadata": {
        "id": "YNhjck7yAmWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = manhattan(train_data_3, test_data_3)\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "eCjoQ2iVBx9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features = 12"
      ],
      "metadata": {
        "id": "bqO0HBp_CrpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = manhattan(train_data_12, test_data_12)\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "0xJPP24BCvsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Euclidian distance\n",
        "\n",
        "We can simply find the euclidian distance from the minskowski distance by just setting the value of q = 2."
      ],
      "metadata": {
        "id": "KPBVH2ejEW33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the euclidian distance\n",
        "q = 2\n",
        "Euclidian_pred = nearest_neighbour(train_data, test_data, q)"
      ],
      "metadata": {
        "id": "9OFf3oEW4EXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calulating the accuracy, recall, precision, f1_score\n",
        "qs = [2]\n",
        "results = cal_scores_for_qs(qs, train_data, test_data)"
      ],
      "metadata": {
        "id": "NNDKtg744EXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.set_index('q', inplace = True)\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "17oMq2J74EXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With features selection data\n",
        "\n",
        "Features = 3"
      ],
      "metadata": {
        "id": "EEzsO1MuCP6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = euclidian(train_data_3, test_data_3)\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "89yJMabjCRBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features = 12"
      ],
      "metadata": {
        "id": "lPaSYs4zChAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = euclidian(train_data_12, test_data_12)\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "ifvIL1TeClmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "H9-kMm2i4bN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without any Feature selection"
      ],
      "metadata": {
        "id": "-XGtqvKUEcFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [i for i in range(1, 21)]\n",
        "results = cal_scores_for_qs(qs, train_data = train_data, test_data = test_data)"
      ],
      "metadata": {
        "id": "0Sc4PG2sAkHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df.set_index('q'))"
      ],
      "metadata": {
        "id": "naJULuyp47WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_evaluation_metric(results_df)"
      ],
      "metadata": {
        "id": "3Lqg1VFEFToo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the figure above, we can observe that;\n",
        "\n",
        " with the k = 1, (simple nearest neighbours), and q = 2, we see the best f1_scores (~ 0.91) with good recall (~ 0.97) and precision (~ 0.86)."
      ],
      "metadata": {
        "id": "WYdv67V9m2GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Feature selection"
      ],
      "metadata": {
        "id": "8hrOFqeHFHhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features = 3"
      ],
      "metadata": {
        "id": "j8UdtECoGMNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [i for i in range(1, 21)]\n",
        "results = cal_scores_for_qs(qs, train_data = train_data_3, test_data = test_data_3)\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df.set_index('q'))"
      ],
      "metadata": {
        "id": "MecoTaliFg3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_evaluation_metric(results_df)"
      ],
      "metadata": {
        "id": "5PxQ7wJCGzKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features = 12"
      ],
      "metadata": {
        "id": "pBxFtcqFGRJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [i for i in range(1, 21)]\n",
        "results = cal_scores_for_qs(qs, train_data = train_data_12, test_data = test_data_12)\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df.set_index('q'))"
      ],
      "metadata": {
        "id": "JA5UzFLnGEWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_evaluation_metric(results_df)"
      ],
      "metadata": {
        "id": "N6TFeu0DG4xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "hSa10Z_E4a_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the results section, it is evident that the optimal max_features for random forest are identified. Additionally, the selection of columns based on correlation yields subpar f1, accuracy, recall, and precision scores compared to using all features for nearest neighbors calculation.\n",
        "\n",
        "Furthermore, it is noted that when max_features is set to 12, representing the 12 most significant features out of 60, encompassing 90% of the training dataset's cumulative variance, the accuracy, precision, recall, and f1_score are all 1. This could potentially indicate overfitting and warrants further consideration."
      ],
      "metadata": {
        "id": "3FcT6jtiEk5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "[1]\n",
        "[IDM - book] Introduction to Data Mining, 2nd edition, Pearson, 2019, by Pang-Ning Tan, Michael Steinbach, Vipin Kumar, Anuj Karpatne\n"
      ],
      "metadata": {
        "id": "vgoJPHjx4mlo"
      }
    }
  ]
}