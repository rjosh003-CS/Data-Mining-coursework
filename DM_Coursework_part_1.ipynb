{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn59ZaH9fieC93h6lQljp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjosh003-CS/Data-Mining-coursework/blob/main/DM_Coursework_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Data Mining Assignment</b><br>\n",
        "\n",
        "<b>Opened:</b> Tuesday, 20 February 2024, 12:00 AM<br>\n",
        "<b>Due:</b> Friday, 22 March 2024, 12:00 PM\n",
        "</b>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "<b>Data Mining Assignment</b> <br>\n",
        "\n",
        "This assignment represents 100% of the Data Mining module’s mark. It is composed of Part 1 which is worth 40 marks, and Part 2 which is worth 60 marks. You can work in a team of 2 students for this assignment. One student per team will be chosen by the team as being the team leader – who will be in charge of coordinating the team’s work, and of submitting the assignment in their account on VLE on behalf of all the team.\n",
        "\n",
        "<b>PART 1:</b>\n",
        "\n",
        "This task is based on the Sonar real data seen previously in class. Several objects which can be rock or metal cylinders are scanned on different angles and under different conditions, with sonar signals. 60 measurements are recorded per columns for each object (one record per object) and these are the predictors called A1, A2, …, A60. The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is metal cylinder, and this is the outcome variable called Class.\n",
        "\n",
        "Two datasets are provided to you: a training dataset in the sonar_train.csv file, and a test dataset in the sonar_test.csv file.\n",
        "\n",
        "a) You are required to write a Python code implementing the simple Nearest Neighbour algorithm, with the Minkowski distance, both discussed in lecture of week 1. You should not implement k-Nearest Neighbour, for an arbitrary number of neighbours k. Your code will read the power q appearing in the Mionkowski distance, and will classify each record from the test dataset based on the training dataset. Remember, to classify a record from the test set you need to find its nearest neighbour in the training set (this is the one which minimizes the distance to the test set record); take the class of the nearest neighbour as the predicted class for the test set record. After classifying all the records in the test set, your code needs to calculate and display the accuracy, recall, precision, and F1 measure with respect to the class \"M\" (which is assumed to be the positive class), of the predictions on the test dataset. Run your code to produce results for Manhattan and for Euclidian distances, which are particular cases of Minkowski's distance.\n",
        "\n",
        "b) Run your code for the power q as a positive integer number from 1 to 20 and display the accuracy, recall, precision, and F1 measure on the test set in a chart. Which value of q leads to the best accuracy on the test set?\n",
        "\n",
        "The code, comments, explanations and results will be provided in a Jupyter notebook called Part1.\n",
        "\n",
        "<u>Note that in this task you are not to apply a library for the nearest neighbour algorithm, but you are required to compute the distances, find the nearest neighbour, and so code yourself this simple algorithm.</u>\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>PART 2:</b>\n",
        "\n",
        "This task is based on a real credit risk data, and is to predict a response variable Y which represents a credit card default payment (Yes = 1, No = 0), using the 23 input variables as follows:\n",
        "\n",
        "X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "X2: Gender (1 = male; 2 = female).\n",
        "X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "X4: Marital status (1 = married; 2 = single; 3 = others).\n",
        "X5: Age (year).\n",
        "X6 - X11: History of past payment. One tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.\n",
        "X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.\n",
        "\n",
        "Two datasets are provided to you: a training dataset in the creditdefault_train.csv file, and a test dataset in the creditdefault_test.csv file.\n",
        "\n",
        "Using Python and any relevant libraries, you are required to build the best predictive model by tuning models using cross validation on the training dataset with each of the following algorithms discussed in this module: <u>k-Nearest Neighbours, Decision Trees, Random Forest, Bagging, AdaBoost, and SVM.</u> You may replace the AdaBoost algorithm with XGBoost algorithm, but do not use both of them. Out of the models tuned with all the above algorithms, select the best model and clearly justify your choice, and evaluate its performances on the test set.\n",
        "\n",
        "The coding, comments and explanations will be provided in your Python Jupyter notebook called Part2, which should include also the results. Moreover, for each algorithm mentioned above, include 1 chart in the notebook illustrating how accuracy of the models vary when you vary the values of one numeric hyperparameter only (at your choice).\n",
        "\n",
        "Note regarding working in a team or individually, and what you need to submit:\n",
        "\n",
        "  * <b>You can work and submit in a team of 2 students </b>- in which case you should choose a team leader.  As a team you should work on all the tasks. Include the names and student numbers of both of the team members on top of each notebooks Part 1 and Part 2, and indicate who is the team leader. The team leader must perform the submission from their account (hence only once) for both students.\n",
        "\n",
        "  * <b>Or you can work also work and submit alone </b>for this coursework. In this case you must tackle only point (a) in Part 1, and only 3 out of the 6 algorithms mentioned in Part 2 (at your choice, but **choose 3 only**). Include your name and student number on top of the notebooks Part 1 and Part 2,  followed by the mention **\"I worked and submitted alone\"**"
      ],
      "metadata": {
        "id": "uoPtBXqk1fMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Coursework summary\n",
        "\n",
        "**Part 1:**\n",
        "- **Objective:** Implement the Nearest Neighbour algorithm with Minkowski distance on the Sonar dataset to classify objects as rock or metal cylinders.\n",
        "- **Tasks:**\n",
        "  - Implement Nearest Neighbour algorithm with Minkowski distance.\n",
        "  - Classify test dataset records based on the training dataset.\n",
        "  - Calculate accuracy, recall, precision, and F1 measure for different values of q (power in Minkowski distance).\n",
        "  - Run the code for Manhattan and Euclidean distances.\n",
        "- **Submission Requirements:** A Jupyter notebook named Part1 with code, comments, explanations, and results displayed.\n",
        "\n",
        "**Part 2:**\n",
        "- **Objective:** Build the best predictive model for credit card default prediction using various machine learning algorithms.\n",
        "- **Tasks:**\n",
        "  - Use k-Nearest Neighbours, Decision Trees, Random Forest, Bagging, AdaBoost (or XGBoost), and SVM algorithms.\n",
        "  - Tune models using cross-validation on the training dataset.\n",
        "  - Select the best model and justify the choice.\n",
        "  - Evaluate the best model's performance on the test set.\n",
        "  - Include charts illustrating how accuracy varies with one numeric hyperparameter for each algorithm.\n",
        "- **Submission Requirements:** A Jupyter notebook named Part2 with code, comments, explanations, and results displayed. Additionally, include charts for hyperparameter tuning.\n",
        "\n",
        "**Submission Details:**\n",
        "- **Teamwork:** Work in teams of two, with one team leader coordinating the submission.\n",
        "- **Individual Work:** If working alone, only tackle Part 1 and three algorithms from Part 2.\n",
        "- **Submission:** Team leader submits the assignment from their account, mentioning both team members' names and student numbers at the top of each notebook. If working alone, include a note indicating so.\n",
        "- **Deadline:** Ensure timely submission as per the course guidelines.\n",
        "\n",
        "Both parts emphasize coding, explanation, and analysis, with the second part requiring the exploration of multiple machine learning algorithms and model tuning for predictive performance."
      ],
      "metadata": {
        "id": "U_Pbd4-a1h1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "not_clonned = True\n",
        "\n",
        "class GIT():\n",
        "\n",
        "  def __init__(self, local_path = None, repo_url = None):\n",
        "    self.github_token = userdata.get('github_token')\n",
        "    self.username = userdata.get('username')\n",
        "    self.dir_name = userdata.get('dir_name')\n",
        "    self.repo_name= userdata.get('repo_name')\n",
        "\n",
        "    print(f\"github_token: {self.github_token}\")\n",
        "    print(f\"username: {self.username}\")\n",
        "    print(f\"dir_name: {self.dir_name}\")\n",
        "    print(f\"repo_name: {self.repo_name}\")\n",
        "\n",
        "    # Construct the repository URL and local path\n",
        "    if repo_url == None:\n",
        "      self.repo_url = f\"https://github.com/{self.username}/{self.repo_name}.git\"\n",
        "    else:\n",
        "      self.repo_url = repo_url\n",
        "\n",
        "    if local_path == None:\n",
        "      self.local_path = f\"/content/{self.repo_name}\"\n",
        "    else:\n",
        "      self.local_path = local_path\n",
        "\n",
        "    print(f\"repo_url: {self.repo_url}\")\n",
        "    print(f\"local_path: {self.local_path}\")\n",
        "\n",
        "\n",
        "  def clone_git(self):\n",
        "      # Clone the repository using the token for authentication\n",
        "      !git clone {self.repo_url} {self.local_path}\n",
        "      !git config --global credential.helper store\n",
        "      !cd {local_path} && git checkout master\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if not_clonned:\n",
        "    print(f\"not_clonned: {not_clonned}\")\n",
        "    git = GIT()\n",
        "    git.clone_git()\n",
        "    not_clonned = False\n",
        "    print(f\"not_clonned: {not_clonned}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKd1Y79e1r2G",
        "outputId": "1cabac2c-f1e6-4945-9589-6c760fd5ab55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not_clonned: True\n",
            "github_token: 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\n",
            "username: rjosh003-CS\n",
            "dir_name: data\n",
            "repo_name: Data-Mining-coursework\n",
            "repo_url: https://github.com/rjosh003-CS/Data-Mining-coursework.git\n",
            "local_path: /content/Data-Mining-coursework\n",
            "Cloning into '/content/Data-Mining-coursework'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 40 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (40/40), 1.14 MiB | 4.83 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "/bin/bash: line 1: cd: {local_path}: No such file or directory\n",
            "not_clonned: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<h1></h1>\n",
        "<h1 align = center><b>Coursework 1 </b><h1>\n",
        "<h2 align = center><u><i>Introduction to Data Mining (2023-2024)</i></u></h2>\n",
        "\n",
        "<h3 align = center>Part 1: Sonar Data</h3>\n",
        "\n",
        "<br>\n",
        "<h4 align = center>Student:  <span style=\"color:blue\">Rohit Joshi</span></h4>\n",
        "<h4 align = center>Student ID:  <span style=\"color:blue\">33726546</span></h4>\n",
        "<h4 align = center>Submission Date:  <span style=\"color:blue\">22th March 2024</span></h4>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LnwVKHnhw6Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Introduction"
      ],
      "metadata": {
        "id": "JjO70jOFzAWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Definition of the problem\n"
      ],
      "metadata": {
        "id": "K4RCs9THzFA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 About the Dataset"
      ],
      "metadata": {
        "id": "gOUSsBAqzV_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Evaluation Metrics"
      ],
      "metadata": {
        "id": "aNWw9y72zcsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Data Processing\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d2PBXRl7zuSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Data Pre-processing"
      ],
      "metadata": {
        "id": "QARYJ8gTzuIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Data Loading"
      ],
      "metadata": {
        "id": "sJ0Qx90vzsEb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iwR188te4_WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Cleaning"
      ],
      "metadata": {
        "id": "U8brMRob0nGj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoIyzVub4_Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Features Engineering"
      ],
      "metadata": {
        "id": "byICrybP0slg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDGE8yYQ4-zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Train-Validation-Test Split"
      ],
      "metadata": {
        "id": "C3lxvvQ40sEs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8UIGl1x4-gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Normalization/Scaling\n",
        "- **Feature Scaling:** Normalize/standardize input features to a similar scale to prevent dominance by certain features during model training.\n",
        "  - Techniques StandardScaler is applied to the features."
      ],
      "metadata": {
        "id": "NJ6FAhKh1lCS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmcNspSd45Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Data Formatting\n",
        "- **Convert to Suitable Format:** Organize the data into sequences or time windows (if using recurrent neural networks or time series models).\n",
        "- **Convert to Arrays or Tensors:** Transform the data into arrays or tensors compatible with the chosen machine learning or deep learning framework."
      ],
      "metadata": {
        "id": "pb3rM3h31jUG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ods5sLNB45xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.7 Data Pipeline Creation (Optional)\n",
        "- **Pipeline Construction:** Construct a data processing pipeline to automate and streamline the preprocessing steps, ensuring consistency across different datasets and reducing code redundancy."
      ],
      "metadata": {
        "id": "WQHxoFhE1jJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxUMHjSj46Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Save Processed Data (optional)\n",
        "- **Save Processed Data:** Save the processed datasets (training, validation, test) in a suitable format (CSV, HDF5, etc.) for easy access during model training and evaluation.\n"
      ],
      "metadata": {
        "id": "4FECqslj0rvA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPDWmpb246ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Methodology"
      ],
      "metadata": {
        "id": "_cejLsIh21J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Finding the Common Sense Baseline"
      ],
      "metadata": {
        "id": "HEJj7I5T3ocS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naJULuyp47WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Developing the First Model with Statistical Power"
      ],
      "metadata": {
        "id": "dCTeex8V31yG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrGXZZmf3Pjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "H9-kMm2i4bN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHE3zJVe42-U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3iOn9EC42i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "hSa10Z_E4a_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference"
      ],
      "metadata": {
        "id": "vgoJPHjx4mlo"
      }
    }
  ]
}